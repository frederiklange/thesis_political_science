{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba095ad7",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274cafa6",
   "metadata": {},
   "source": [
    "### Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b838bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import time\n",
    "from joblib import parallel_backend\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn import metrics # Used to get performance metrics from models\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "# Validation of models\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import scipy\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# for vectorization \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import scipy.sparse\n",
    "\n",
    "# Import google\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68dd6fc",
   "metadata": {},
   "source": [
    "## Elements to define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of ys\n",
    "requested_cols = ['blok',\n",
    "                  'KF',\n",
    "                  'V',\n",
    "                  'S',\n",
    "                  'CD',\n",
    "                  'RV',\n",
    "                  'FP',\n",
    "                  'SF',\n",
    "                  'KD',\n",
    "                  'DF',\n",
    "                  'EL',\n",
    "                  'LA',\n",
    "                  'ALT',\n",
    "                  'NB'\n",
    "                 ]\n",
    "\n",
    "# Creating list of random ys\n",
    "requested_cols_random = ['blok_random',\n",
    "                         'KF_random',\n",
    "                         'V_random',\n",
    "                         'S_random',\n",
    "                         'CD_random',\n",
    "                         'RV_random', \n",
    "                         'FP_random', \n",
    "                         'SF_random', \n",
    "                         'KD_random', \n",
    "                         'DF_random', \n",
    "                         'EL_random', \n",
    "                         'LA_random', \n",
    "                         'ALT_random', \n",
    "                         'NB_random', \n",
    "                         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0efe57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function\n",
    "def create_x_y(dataframe):\n",
    "    \n",
    "    # subset features to tf-idf vectors\n",
    "    X = dataframe.iloc[:, dataframe.columns.str.find(\"no_words_after\").argmax()+1:]\n",
    "    \n",
    "    # Valid ys\n",
    "    valid_ys = [c for c in requested_cols if c in dataframe]\n",
    "    valid_ys_random = [c for c in requested_cols_random if c in dataframe]\n",
    "    \n",
    "    ys = dataframe[valid_ys]\n",
    "    \n",
    "    y_rands = dataframe[valid_ys_random]\n",
    "    \n",
    "    # Modify X - making it sparse\n",
    "    #X = scipy.sparse.csr_matrix(X.sparse.to_coo())\n",
    "    \n",
    "    # creating lists\n",
    "    y_list = ys.columns\n",
    "    y_rand_list = y_rands.columns\n",
    "    \n",
    "    return X, ys, y_rands, y_list, y_rand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa2b28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating evaluation function\n",
    "def model_evaluation_df(y_train, y_train_pred, y_val, y_val_pred):\n",
    "    '''\n",
    "    Calculates evaluation metrics for a training and validation set.\n",
    "        \n",
    "        Parameters:\n",
    "            X_train (dataframe): Training set with the features that model uses\n",
    "            y_train (dataframe): Training set with the targets that model uses\n",
    "            X_val (dataframe): Validation set with the features that model uses\n",
    "            y_val (dataframe): Validation set with the targets that model uses\n",
    "            \n",
    "        Returns:\n",
    "            model_evaluation_dict (dictionary): Dictionary with scores for the following performance metrics: AUC, AP, BS, Precision, Recall, FP rate and TP rate\n",
    "    '''\n",
    "    \n",
    "    # compute the performance metrics for the training set\n",
    "    accuracy_train = metrics.accuracy_score(y_train, y_train_pred.round())\n",
    "    AUC_train = metrics.roc_auc_score(y_train, y_train_pred)\n",
    "    AP_train = metrics.average_precision_score(y_train, y_train_pred)\n",
    "    BS_train = metrics.brier_score_loss(y_train, y_train_pred)\n",
    "    precision_train, recall_train, _ = metrics.precision_recall_curve(y_train, y_train_pred)\n",
    "    FP_rate_train, TP_rate_train, _ = metrics.roc_curve(y_train, y_train_pred)\n",
    "\n",
    "    # compute the performance metrics for the validation set\n",
    "    accuracy_val = metrics.accuracy_score(y_val, y_val_pred.round())\n",
    "    AUC_val = metrics.roc_auc_score(y_val, y_val_pred)\n",
    "    AP_val = metrics.average_precision_score(y_val, y_val_pred)\n",
    "    BS_val = metrics.brier_score_loss(y_val, y_val_pred)\n",
    "    precision_val, recall_val, _ = metrics.precision_recall_curve(y_val, y_val_pred) \n",
    "    FP_rate_val, TP_rate_val, _ = metrics.roc_curve(y_val, y_val_pred)\n",
    "    \n",
    "    # Makes a dictionary of all the metrics\n",
    "    model_evaluation_dict = {\"accuracy_train\": [accuracy_train],\n",
    "                             \"auc_train\": [AUC_train],\n",
    "                             \"ap_train\": [AP_train],\n",
    "                             \"bs_train\": [BS_train],\n",
    "                             \"precision_train\": [precision_train],\n",
    "                             \"recall_train\": [recall_train],\n",
    "                             \"FP_rate_train\": [FP_rate_train],\n",
    "                             \"TP_rate_train\": [TP_rate_train],\n",
    "                             \"accuracy_test\": [accuracy_val],\n",
    "                             \"auc_test\": [AUC_val],\n",
    "                             \"ap_test\": [AP_val],\n",
    "                             \"bs_test\": [BS_val],\n",
    "                             \"precision_test\": [precision_val],\n",
    "                             \"recall_test\": [recall_val],\n",
    "                             \"FP_rate_test\": [FP_rate_val],\n",
    "                             \"TP_rate_test\": [TP_rate_val]\n",
    "                            }\n",
    "\n",
    "    return model_evaluation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10773a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitter(X,y):\n",
    "    #Splitting into test and dev data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/5, random_state=42)    \n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fd727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_scores = ['blok_scores',\n",
    "                  'KF_scores',\n",
    "                  'V_scores',\n",
    "                  'S_scores',\n",
    "                  'CD_scores',\n",
    "                  'RV_scores',\n",
    "                  'FP_scores',\n",
    "                  'SF_scores',\n",
    "                  'KD_scores',\n",
    "                  'DF_scores',\n",
    "                  'EL_scores',\n",
    "                  'LA_scores',\n",
    "                  'ALT_scores',\n",
    "                  'NB_scores'\n",
    "                 ]\n",
    "\n",
    "list_of_scores_random = ['blok_random_scores',\n",
    "                         'KF_random_scores',\n",
    "                         'V_random_scores',\n",
    "                         'S_random_scores',\n",
    "                         'CD_random_scores',\n",
    "                         'RV_random_scores', \n",
    "                         'FP_random_scores', \n",
    "                         'SF_random_scores', \n",
    "                         'KD_random_scores', \n",
    "                         'DF_random_scores', \n",
    "                         'EL_random_scores', \n",
    "                         'LA_random_scores', \n",
    "                         'ALT_random_scores', \n",
    "                         'NB_random_scores', \n",
    "                         ]\n",
    "\n",
    "list_of_preds = ['blok_preds',\n",
    "                  'KF_preds',\n",
    "                  'V_preds',\n",
    "                  'S_preds',\n",
    "                  'CD_preds',\n",
    "                  'RV_preds',\n",
    "                  'FP_preds',\n",
    "                  'SF_preds',\n",
    "                  'KD_preds',\n",
    "                  'DF_preds',\n",
    "                  'EL_preds',\n",
    "                  'LA_preds',\n",
    "                  'ALT_preds',\n",
    "                  'NB_preds'\n",
    "                 ]\n",
    "\n",
    "list_of_random_preds = ['blok_random_preds',\n",
    "                         'KF_random_preds',\n",
    "                         'V_random_preds',\n",
    "                         'S_random_preds',\n",
    "                         'CD_random_preds',\n",
    "                         'RV_random_preds', \n",
    "                         'FP_random_preds', \n",
    "                         'SF_random_preds', \n",
    "                         'KD_random_preds', \n",
    "                         'DF_random_preds', \n",
    "                         'EL_random_preds', \n",
    "                         'LA_random_preds', \n",
    "                         'ALT_random_preds', \n",
    "                         'NB_random_preds', \n",
    "                         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06efb198",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527528bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_speeches_yearly_tfids_dfs = pd.read_pickle('gs://speciale_ml/all_speeches_yearly_tfids_dfs.pkl')\n",
    "velfærd_speeches_yearly_tfids_dfs = pd.read_pickle('gs://speciale_ml/velfærd_speeches_yearly_tfids_dfs.pkl')\n",
    "skat_speeches_yearly_tfids_dfs = pd.read_pickle('gs://speciale_ml/skat_speeches_yearly_tfids_dfs.pkl')\n",
    "klima_speeches_yearly_tfids_dfs = pd.read_pickle('gs://speciale_ml/klima_speeches_yearly_tfids_dfs.pkl')\n",
    "udlændinge_speeches_yearly_tfids_dfs = pd.read_pickle('gs://speciale_ml/udlændinge_speeches_yearly_tfids_dfs.pkl')\n",
    "scores_df_empty = pd.read_pickle('gs://speciale_ml/scores_df_empty.pkl')\n",
    "preds_df_empty = pd.read_pickle('gs://speciale_ml/preds_df_empty.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fc7075",
   "metadata": {},
   "source": [
    "## Small data adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a194ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for list_df in [udlændinge_speeches_yearly_tfids_dfs,\n",
    "                all_speeches_yearly_tfids_dfs,\n",
    "                klima_speeches_yearly_tfids_dfs,\n",
    "                velfærd_speeches_yearly_tfids_dfs,\n",
    "                skat_speeches_yearly_tfids_dfs\n",
    "               ]:\n",
    "    \n",
    "    for df in list_df:\n",
    "        cols = []\n",
    "        count = 1\n",
    "        \n",
    "        for column in df.columns:\n",
    "            if column == 'blok':\n",
    "                cols.append(f'blok{count}')\n",
    "                count+=1\n",
    "                continue\n",
    "            cols.append(column)\n",
    "        df.columns = cols\n",
    "\n",
    "        df.rename(columns={'blok1': 'blok'}, inplace=True)\n",
    "\n",
    "        df['blok']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a2a14",
   "metadata": {},
   "source": [
    "## Naive Bayes - models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df895723",
   "metadata": {},
   "source": [
    "### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825edb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(all_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "NB_results_all = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        NB_yearly_scores = []\n",
    "        NB_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in all_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = ComplementNB()\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "                          }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = GridSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            NB_yearly_scores.append(scores_df)\n",
    "            NB_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        NB_results_all.update({f'{ele_y}_scores': NB_yearly_scores,\n",
    "                               f'{ele_y}_preds': NB_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4a06e2",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dc2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(NB_results_all[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('NB_all_speeches_plot.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in NB_results_all[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('NB_all_speeches_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddadf19f",
   "metadata": {},
   "source": [
    "### Velfærd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b19075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(velfærd_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "NB_results_velfærd = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        NB_yearly_scores = []\n",
    "        NB_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in velfærd_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = ComplementNB()\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "                          }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = GridSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            NB_yearly_scores.append(scores_df)\n",
    "            NB_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        NB_results_velfærd.update({f'{ele_y}_scores': NB_yearly_scores,\n",
    "                               f'{ele_y}_preds': NB_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c168d",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(NB_results_velfærd[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('NB_velfærd_speeches_plot.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d7f4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in NB_results_velfærd[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('NB_velfærd_speeches_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc8917",
   "metadata": {},
   "source": [
    "### Skat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c21cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(skat_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "NB_results_skat = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        NB_yearly_scores = []\n",
    "        NB_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in skat_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = ComplementNB()\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "                          }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = GridSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            NB_yearly_scores.append(scores_df)\n",
    "            NB_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        NB_results_skat.update({f'{ele_y}_scores': NB_yearly_scores,\n",
    "                               f'{ele_y}_preds': NB_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3458a69b",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62205620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(NB_results_skat[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('NB_skat_speeches_plot.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in NB_results_skat[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('NB_skat_speeches_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcceb0f2",
   "metadata": {},
   "source": [
    "### Udlændinge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72226e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(udlændinge_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "NB_results_udlændinge = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        NB_yearly_scores = []\n",
    "        NB_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in udlændinge_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = ComplementNB()\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "                          }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = GridSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            NB_yearly_scores.append(scores_df)\n",
    "            NB_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        NB_results_udlændinge.update({f'{ele_y}_scores': NB_yearly_scores,\n",
    "                               f'{ele_y}_preds': NB_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbd5320",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e424c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(NB_results_udlændinge[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('NB_udlændinge_speeches_plot.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7123e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in NB_results_udlændinge[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('NB_udlændinge_speeches_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188fd94",
   "metadata": {},
   "source": [
    "### Klima data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff674ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(klima_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "NB_results_klima = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        NB_yearly_scores = []\n",
    "        NB_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in klima_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = ComplementNB()\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "                          }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = GridSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            NB_yearly_scores.append(scores_df)\n",
    "            NB_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        NB_results_klima.update({f'{ele_y}_scores': NB_yearly_scores,\n",
    "                               f'{ele_y}_preds': NB_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e95dd81",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93c3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(NB_results_klima[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('NB_klima_speeches_plot.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663890b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in NB_results_klima[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('NB_klima_speeches_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67c69e",
   "metadata": {},
   "source": [
    "## Naive Bayes Random - models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60683ba",
   "metadata": {},
   "source": [
    "### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c8bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(all_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "NB_results_all = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        NB_yearly_scores = []\n",
    "        NB_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in all_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_rand_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = ComplementNB()\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "                          }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = GridSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            NB_yearly_scores.append(scores_df)\n",
    "            NB_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        NB_results_all.update({f'{ele_y}_scores': NB_yearly_scores,\n",
    "                               f'{ele_y}_preds': NB_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05f657",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(NB_results_all[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('NB_all_speeches_plot_rand.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1d493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in NB_results_all[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('NB_all_speeches_preds_rand.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e0843",
   "metadata": {},
   "source": [
    "### Velfærd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ee87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(velfærd_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "NB_results_velfærd = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        NB_yearly_scores = []\n",
    "        NB_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in velfærd_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_rand_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = ComplementNB()\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "                          }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = GridSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            NB_yearly_scores.append(scores_df)\n",
    "            NB_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        NB_results_velfærd.update({f'{ele_y}_scores': NB_yearly_scores,\n",
    "                               f'{ele_y}_preds': NB_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d4d94",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8088cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(NB_results_velfærd[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('NB_velfærd_speeches_plot_rand.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9babaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in NB_results_velfærd[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('NB_velfærd_speeches_preds_rand.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12e2237",
   "metadata": {},
   "source": [
    "### Skat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a93cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(skat_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "NB_results_skat = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        NB_yearly_scores = []\n",
    "        NB_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in skat_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_rand_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = ComplementNB()\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "                          }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = GridSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            NB_yearly_scores.append(scores_df)\n",
    "            NB_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        NB_results_skat.update({f'{ele_y}_scores': NB_yearly_scores,\n",
    "                               f'{ele_y}_preds': NB_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6473fa",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ebdc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(NB_results_skat[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('NB_skat_speeches_plot.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aa74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in NB_results_skat[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('NB_skat_speeches_preds_rand.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8262c94",
   "metadata": {},
   "source": [
    "### Udlændinge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb354ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(udlændinge_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "NB_results_udlændinge = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        NB_yearly_scores = []\n",
    "        NB_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in udlændinge_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_rand_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = ComplementNB()\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "                          }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = GridSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            NB_yearly_scores.append(scores_df)\n",
    "            NB_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        NB_results_udlændinge.update({f'{ele_y}_scores': NB_yearly_scores,\n",
    "                               f'{ele_y}_preds': NB_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4324a62",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee80eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(NB_results_udlændinge[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('NB_udlændinge_speeches_plot_rand.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in NB_results_udlændinge[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('NB_udlændinge_speeches_preds_rand.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738030e0",
   "metadata": {},
   "source": [
    "### Klima data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(klima_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "NB_results_klima = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        NB_yearly_scores = []\n",
    "        NB_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in klima_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_rand_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = ComplementNB()\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {'alpha': [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "                          }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = GridSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            NB_yearly_scores.append(scores_df)\n",
    "            NB_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        NB_results_klima.update({f'{ele_y}_scores': NB_yearly_scores,\n",
    "                               f'{ele_y}_preds': NB_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc379a",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ac63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(NB_results_klima[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('NB_klima_speeches_plot_rand.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in NB_results_klima[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('NB_klima_speeches_preds_rand.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a1b854",
   "metadata": {},
   "source": [
    "## Random Forest - models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2d6aaf",
   "metadata": {},
   "source": [
    "### All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb56e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(all_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "RF_results_all = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        RF_yearly_scores = []\n",
    "        RF_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in all_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = RandomForestClassifier(class_weight=\"balanced\")\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {\"n_estimators\": [16,32,64,128],\n",
    "                          \"max_depth\": [3,5,7,9],\n",
    "                          \"min_samples_split\": [2,4,6],\n",
    "                          \"min_samples_leaf\": [1,2,4]\n",
    "                         }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = RandomizedSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            RF_yearly_scores.append(scores_df)\n",
    "            RF_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        RF_results_all.update({f'{ele_y}_scores': RF_yearly_scores,\n",
    "                               f'{ele_y}_preds': RF_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f53cb",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb928b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(RF_results_all[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('RF_all_speeches_plot.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in RF_results_all[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('RF_all_speeches_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7974d",
   "metadata": {},
   "source": [
    "### Velfærd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783dede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(velfærd_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "RF_results_velfærd = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        RF_yearly_scores = []\n",
    "        RF_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in velfærd_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = RandomForestClassifier(class_weight=\"balanced\")\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {\"n_estimators\": [16,32,64,128],\n",
    "                          \"max_depth\": [3,5,7,9],\n",
    "                          \"min_samples_split\": [2,4,6],\n",
    "                          \"min_samples_leaf\": [1,2,4]\n",
    "                         }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = RandomizedSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            RF_yearly_scores.append(scores_df)\n",
    "            RF_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        RF_results_velfærd.update({f'{ele_y}_scores': RF_yearly_scores,\n",
    "                               f'{ele_y}_preds': RF_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97521792",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55594c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(RF_results_velfærd[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('RF_velfærd_speeches_plot.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccf76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in RF_results_velfærd[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('RF_velfærd_speeches_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1edf1f",
   "metadata": {},
   "source": [
    "### Skat data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cf092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(skat_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "RF_results_skat = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        RF_yearly_scores = []\n",
    "        RF_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in skat_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = RandomForestClassifier(class_weight=\"balanced\")\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {\"n_estimators\": [16,32,64,128],\n",
    "                          \"max_depth\": [3,5,7,9],\n",
    "                          \"min_samples_split\": [2,4,6],\n",
    "                          \"min_samples_leaf\": [1,2,4]\n",
    "                         }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = RandomizedSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            RF_yearly_scores.append(scores_df)\n",
    "            RF_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        RF_results_skat.update({f'{ele_y}_scores': RF_yearly_scores,\n",
    "                               f'{ele_y}_preds': RF_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4595a627",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cab863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(RF_results_skat[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('RF_skat_speeches_plot.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in RF_results_skat[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('RF_skat_speeches_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9b228b",
   "metadata": {},
   "source": [
    "### Udlændinge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51736b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(udlændinge_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "RF_results_udlændinge = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        RF_yearly_scores = []\n",
    "        RF_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in udlændinge_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = RandomForestClassifier(class_weight=\"balanced\")\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {\"n_estimators\": [16,32,64,128],\n",
    "                          \"max_depth\": [3,5,7,9],\n",
    "                          \"min_samples_split\": [2,4,6],\n",
    "                          \"min_samples_leaf\": [1,2,4]\n",
    "                         }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = RandomizedSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            RF_yearly_scores.append(scores_df)\n",
    "            RF_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        RF_results_udlændinge.update({f'{ele_y}_scores': RF_yearly_scores,\n",
    "                               f'{ele_y}_preds': RF_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff44bfa4",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a3990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(RF_results_udlændinge[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('RF_udlændinge_speeches_plot.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d9205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in RF_results_udlændinge[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('RF_udlændinge_speeches_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85aa6d",
   "metadata": {},
   "source": [
    "### Klima data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1f7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and ys\n",
    "_, _, _, y_list, y_list_rand = create_x_y(klima_speeches_yearly_tfids_dfs[0])\n",
    "\n",
    "# Create dicionary with predictions from each comparison\n",
    "RF_results_klima = {}\n",
    "\n",
    "with parallel_backend('multiprocessing',n_jobs=16):\n",
    "\n",
    "    # Looping over different y values\n",
    "    for ele_y, ele_y_rand in zip(y_list, y_list_rand):\n",
    "\n",
    "        # Start time one comparison\n",
    "        start_time_comparison = time.time()\n",
    "\n",
    "        # Creating yearly scores\n",
    "        RF_yearly_scores = []\n",
    "        RF_yearly_preds = []\n",
    "\n",
    "        # Looping over alle the years\n",
    "        for yearly_df in klima_speeches_yearly_tfids_dfs:\n",
    "\n",
    "            # Start time one year\n",
    "            start_time_year = time.time()\n",
    "\n",
    "            # Create yearly X and Ys\n",
    "            X, ys, y_rands, _, _ = create_x_y(yearly_df)\n",
    "            y_yearly = ys[str(ele_y)]\n",
    "            y_rand_yearly = y_rands[str(ele_y_rand)]\n",
    "            X_yearly = X\n",
    "            \n",
    "            #Creating dicts\n",
    "            dict_temp = {}\n",
    "            predictions_dict = {'preds_train': [], 'preds_test': [], 'best_model': [], 'index_original': []}\n",
    "            \n",
    "            # If there is no data for the year\n",
    "            if y_yearly.sum() == 0:\n",
    "                \n",
    "                #Empyt DFs\n",
    "                scores_df = scores_df_empty\n",
    "                predictions_df = preds_df_empty\n",
    "            \n",
    "            else:\n",
    "                # Split data into train and test\n",
    "                x_train, x_test, y_train, y_test = train_test_split(X_yearly, y_yearly, test_size = 0.25, random_state=42)\n",
    "\n",
    "                # Initiatite cross validation\n",
    "                cv = StratifiedKFold(n_splits=5, shuffle = True)\n",
    "\n",
    "                # Define the model\n",
    "                model = RandomForestClassifier(class_weight=\"balanced\")\n",
    "\n",
    "                # Set paramater ranges\n",
    "                params = {\"n_estimators\": [16,32,64,128],\n",
    "                          \"max_depth\": [3,5,7,9],\n",
    "                          \"min_samples_split\": [2,4,6],\n",
    "                          \"min_samples_leaf\": [1,2,4]\n",
    "                         }\n",
    "\n",
    "                # Setting up grid_search\n",
    "                search = RandomizedSearchCV(model, params, scoring='roc_auc', cv=cv, refit=True)\n",
    "\n",
    "                # Fitting the best model\n",
    "                result = search.fit(x_train, y_train)\n",
    "\n",
    "                # Getting the best model\n",
    "                best_model = result.best_estimator_\n",
    "\n",
    "                # Predicting probability\n",
    "                y_train_pred = best_model.predict_proba(x_train)[:,1]\n",
    "                y_test_pred = best_model.predict_proba(x_test)[:,1]\n",
    "\n",
    "                # Appending scores\n",
    "                dict_temp = model_evaluation_df(y_train, y_train_pred, y_test, y_test_pred)\n",
    "\n",
    "                # Creating data frame with predicted values\n",
    "                predictions_dict['preds_train'].append(y_train_pred)\n",
    "                predictions_dict['preds_test'].append(y_test_pred)\n",
    "                predictions_dict['best_model'].append(best_model)\n",
    "                predictions_dict['index_original'].append(y_test.index.values)\n",
    "            \n",
    "                # Creating Data Frame with metrics                 \n",
    "                scores_df = pd.DataFrame.from_dict(dict_temp, orient = 'columns')\n",
    "                predictions_df = pd.DataFrame.from_dict(predictions_dict, orient = 'columns')\n",
    "\n",
    "            # Appending monthly scores\n",
    "            RF_yearly_scores.append(scores_df)\n",
    "            RF_yearly_preds.append(predictions_df)\n",
    "            \n",
    "            # Print time for one year\n",
    "            end_time_year = time.time()\n",
    "            run_time = (end_time_year - start_time_year)/60\n",
    "            print(f'minutes to run one year: {run_time:.2}')\n",
    "\n",
    "        # Saving the results from one comparison\n",
    "        RF_results_klima.update({f'{ele_y}_scores': RF_yearly_scores,\n",
    "                               f'{ele_y}_preds': RF_yearly_preds})\n",
    "\n",
    "        # One comparison time\n",
    "        end_time_comparison = time.time()\n",
    "        run_time = (end_time_comparison - start_time_comparison)/60\n",
    "        print(f'minutes to run {ele_y}: {run_time:.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacbc9cd",
   "metadata": {},
   "source": [
    "Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_scores = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_scores:\n",
    "    \n",
    "    one_group = pd.concat(RF_results_klima[group], ignore_index=True)\n",
    "    one_group['model'] = group \n",
    "\n",
    "    # Appending df to a list\n",
    "    group_scores.append(one_group)\n",
    "    \n",
    "with open('RF_klima_speeches_plot.pkl', 'wb') as f:\n",
    "    pickle.dump(group_scores, f, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25bafdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of dataframes with scores for each group\n",
    "group_preds = []\n",
    "\n",
    "# Loop over dictionary\n",
    "for group in list_of_preds:\n",
    "    \n",
    "    # Create list with df for each year\n",
    "    preds_yearly = []\n",
    "    \n",
    "    # Loop over year\n",
    "    for year in RF_results_klima[group]:\n",
    "        \n",
    "        # Create new df\n",
    "        preds_test = list(year['preds_test'].explode())\n",
    "        index_org = list(year['index_original'].explode())\n",
    "        dict_temp = {\"preds_test\": preds_test,\n",
    "                     \"index_org\": index_org}\n",
    "        # Create df\n",
    "        preds_yearly.append(pd.DataFrame(dict_temp))\n",
    "        \n",
    "    # Appending df to a list\n",
    "    group_preds.append(preds_yearly)\n",
    "\n",
    "with open('RF_klima_speeches_preds.pkl', 'wb') as f:\n",
    "    pickle.dump(group_preds, f, protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
